{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing with an RNN  \n",
    "  \n",
    "In order to get the RNN to consume text data, we need two things:  \n",
    "  \n",
    "1. A list of all the texts, eg `texts = ['Today is a nice day', 'Yesterday was gorgeous',....]`  \n",
    "2. We need the labels in a list too, eg `labels = [1, 0, 1, 1,...]`.  If labels are text string categories, then use one-hot encoding to convert them into an array, with each row of the array corresponding to an observation.  \n",
    "  \n",
    "We first set up a tokenizer as follows:  \n",
    "  \n",
    "```python\n",
    "from keras.preprocessing.text import Tokenizer  \n",
    "from keras.preprocessing.sequence import pad_sequences  \n",
    "  \n",
    "tokenizer = Tokenizer(num_words = vocab_size) #Sets up a blank tokenizer  \n",
    "tokenizer.fit_on_texts(texts)   \n",
    "# fit_on_texts creates the vocabulary based on `texts`, based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words because they appear a lot).  \n",
    "  \n",
    "sequences = tokenizer.texts_to_sequences(texts)  \n",
    "# This step transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary.   \n",
    "  \n",
    "word_index = tokenizer.word_index # This is the dictionary of words and their indexes which one can see using  word_index.items()  \n",
    "  \n",
    "data = pad_sequences(sequences, maxlen = maxlen) # Here we make all texts equal in length, and padding is pre (can be changed   by padding='post')  \n",
    "  \n",
    "labels = np.asarray(labels) # We convert the labels into an array from its original list form.  \n",
    "\n",
    "```  \n",
    "  \n",
    "At this point all our input text data exists as perfectly sized tensors in the variable called `data`, and `labels`.  The `data` variable has references to the `word_index` which is essentially the dictionary for the task.  \n",
    "  \n",
    "Next, we need to create the embeddings_matrix based on Glove.  For this, we first load up Glove in a giant matrix, and then we look up each item in the `word_index` in the Glove matrix and create our pre-trained embeddings_matrix based on Glove.  In this matrix, we preserve the index order as in the `word_index`, so what is #3 in the word index is also the 3rd row in the embeddings_matrix.  The 0th row is all zeros, and only the first row onwards are used because in the `word_index` dictionary there is no entry for zero.  \n",
    "  \n",
    "Now we are good to go to feed this into our NN. The pretrained embeddings_matrix makes its way into the NN using the following commands:  \n",
    "  \n",
    "```python\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "```\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'summary_x', 'URL', 'published', 'keywords', 'summary_y',\n",
       "       'text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "df = joblib.load('final_df3.pkl')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (np.random.randint(0,2,970))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# imdb_dir = '\\\\Users\\\\user\\\\aclImdb'\n",
    "# train_dir = os.path.join(imdb_dir, 'train')\n",
    "# labels=[]\n",
    "# texts=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the text files\n",
    "# for label_type in ['neg','pos']:\n",
    "#     dir_name = os.path.join(train_dir, label_type)\n",
    "#     for fname in os.listdir(dir_name):\n",
    "#         if fname[-4:]== '.txt':\n",
    "#             f = open(os.path.join(dir_name, fname))\n",
    "#             #print(fname)\n",
    "#             texts.append(f.read())\n",
    "#             f.close()\n",
    "#             if label_type =='neg':\n",
    "#                 labels.append(0)\n",
    "#             else:\n",
    "#                 labels.append(1)\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the variable `texts` contains all the reviews in plaintext as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in texts is 970 \n",
      " Its stats are:\n",
      "count     970.000000\n",
      "mean      595.460825\n",
      "std       543.870539\n",
      "min         1.000000\n",
      "25%       275.000000\n",
      "50%       472.500000\n",
      "75%       828.750000\n",
      "max      7153.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f0308e6908>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Number of items in texts is', len(texts), '\\n Its stats are:')\n",
    "print(pd.Series([len(texts[i].split(' ')) for i in range(len(texts))]).describe())\n",
    "pd.Series([len(texts[i].split(' ')) for i in range(len(texts))]).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28836 unique tokens\n",
      "Shape of data tensor (970, 5000)\n",
      "Shape of label tensor (970,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen=5000  # how many words to take from each text\n",
    "training_samples=770\n",
    "validation_samples = 200\n",
    "vocab_size=20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen = maxlen)\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor', data.shape)\n",
    "print('Shape of label tensor', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples:training_samples+validation_samples]\n",
    "y_val = labels[training_samples:training_samples+validation_samples]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "970"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 words and corresponding vectors\n"
     ]
    }
   ],
   "source": [
    "glove_dir = '\\\\Users\\\\user\\\\glove.6B'\n",
    "\n",
    "embeddings_index = {}\n",
    "#f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "f=open('\\\\Users\\\\user\\\\glove.6B\\\\glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s words and corresponding vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index.get('security'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_index.get('th13e'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedding matrix based on Glove\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the `embedding_matrix` has one row per word in the vocabulary.  Each row has the vector for that word, picked from glove.  Because it is an np.array, it has no row or column names. The order of the words in the rows is the same as the order of words in the dict word_index.  \n",
    "  \n",
    "We will feed this embedding matrix as weights to the embedding layer.  \n",
    "  \n",
    "## Build the model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, LSTM, SimpleRNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,017,057\n",
      "Trainable params: 2,017,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim)) # Note that vocab_size=10000 (vocab size), embedding_dim = 100 (100 dense vector for each word from Glove), maxlen=100 (using only first 100 words of each review)\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 616 samples, validate on 154 samples\n",
      "Epoch 1/15\n",
      "616/616 [==============================] - 34s 55ms/step - loss: 0.7146 - acc: 0.4756 - val_loss: 0.6942 - val_acc: 0.5195\n",
      "Epoch 2/15\n",
      "616/616 [==============================] - 29s 47ms/step - loss: 0.6939 - acc: 0.5130 - val_loss: 0.6951 - val_acc: 0.5195\n",
      "Epoch 3/15\n",
      "616/616 [==============================] - 31s 50ms/step - loss: 0.6856 - acc: 0.5390 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 4/15\n",
      "616/616 [==============================] - 31s 50ms/step - loss: 0.6803 - acc: 0.5763 - val_loss: 0.6934 - val_acc: 0.5714\n",
      "Epoch 5/15\n",
      "616/616 [==============================] - 32s 52ms/step - loss: 0.6770 - acc: 0.5844 - val_loss: 0.6961 - val_acc: 0.5065\n",
      "Epoch 6/15\n",
      "616/616 [==============================] - 31s 51ms/step - loss: 0.6739 - acc: 0.5763 - val_loss: 0.7042 - val_acc: 0.4740\n",
      "Epoch 7/15\n",
      "616/616 [==============================] - 33s 53ms/step - loss: 0.6704 - acc: 0.5860 - val_loss: 0.6940 - val_acc: 0.5325\n",
      "Epoch 8/15\n",
      "616/616 [==============================] - 33s 53ms/step - loss: 0.6620 - acc: 0.6185 - val_loss: 0.6926 - val_acc: 0.5195\n",
      "Epoch 9/15\n",
      "616/616 [==============================] - 33s 53ms/step - loss: 0.6578 - acc: 0.6299 - val_loss: 0.6944 - val_acc: 0.5519\n",
      "Epoch 10/15\n",
      "616/616 [==============================] - 33s 54ms/step - loss: 0.6579 - acc: 0.6331 - val_loss: 0.6940 - val_acc: 0.5390\n",
      "Epoch 11/15\n",
      "616/616 [==============================] - 32s 53ms/step - loss: 0.6510 - acc: 0.6429 - val_loss: 0.7014 - val_acc: 0.4740\n",
      "Epoch 12/15\n",
      "616/616 [==============================] - 33s 53ms/step - loss: 0.6473 - acc: 0.6347 - val_loss: 0.6931 - val_acc: 0.5584\n",
      "Epoch 13/15\n",
      "616/616 [==============================] - 32s 53ms/step - loss: 0.6399 - acc: 0.6672 - val_loss: 0.6949 - val_acc: 0.5584\n",
      "Epoch 14/15\n",
      "616/616 [==============================] - 33s 54ms/step - loss: 0.6343 - acc: 0.6526 - val_loss: 0.6923 - val_acc: 0.5649\n",
      "Epoch 15/15\n",
      "616/616 [==============================] - 33s 54ms/step - loss: 0.6310 - acc: 0.6575 - val_loss: 0.6929 - val_acc: 0.5649\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=15, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
